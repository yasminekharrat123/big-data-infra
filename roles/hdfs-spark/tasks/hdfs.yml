- name: "[HDFS] Zip spark_scripts into app.zip"
  archive:
    path: "{{ role_path }}/files/spark_scripts/"
    dest: "{{ role_path }}/files/spark_scripts/app.zip"
    format: zip
  delegate_to: localhost

- name: "[HDFS] Ensure Docker network '{{ hadoop_network }}' exists"
  become: true
  community.docker.docker_network:
    name: "{{ hadoop_network }}"
    driver: bridge
    state: present

- name: "[HDFS] Pull Hadoop image '{{ hadoop_image }}'"
  become: true
  community.docker.docker_image:
    name: "{{ hadoop_image }}"
    source: pull

- name: "Prepare Spark scripts on Docker host"
  become: true
  block:

    - name: "Ensure /spark_scripts exists on the host"
      ansible.builtin.file:
        path: /spark_scripts
        state: directory
        mode: '0755'
        owner: root
        group: root

    - name: "Copy Spark scripts to Docker host"
      become: true
      ansible.builtin.copy:
        src: spark_scripts/
        dest: /spark_scripts/
        mode: '0644'
        owner: root
        group: root

- name: "[HDFS] Run Hadoop master container"
  become: true
  community.docker.docker_container:
    name: "{{ master_container.name }}"
    hostname: "{{ master_container.hostname }}"
    volumes:
      - "/spark_scripts:/spark_scripts:rw"
    image: "{{ hadoop_image }}"
    networks:
      - name: "{{ hadoop_network }}"
    published_ports: "{{ master_container.ports }}"
    detach: true
    interactive: true
    tty: true
    restart_policy: unless-stopped

- name: "Debug: List /spark_scripts inside container"
  become: true
  community.docker.docker_container_exec:
    container: "{{ master_container.name }}"
    command: "ls -l /spark_scripts"
  register: ls_scripts
  changed_when: false

- debug:
    var: ls_scripts.stdout_lines

- name: "[HDFS] Run Hadoop worker containers"
  become: true
  loop: "{{ workers }}"
  loop_control:
    label: "{{ item.name }}"
  community.docker.docker_container:
    name: "{{ item.name }}"
    hostname: "{{ item.hostname }}"
    image: "{{ hadoop_image }}"
    networks:
      - name: "{{ hadoop_network }}"
    published_ports:
      - "{{ item.port }}:8042"
    detach: true
    interactive: true
    tty: true
    restart_policy: unless-stopped

- name: "[HDFS] Wait for master container to be ready"
  become: true
  community.docker.docker_container_info:
    name: "{{ master_container.name }}"
  register: master_container_info
  until: master_container_info.container.State.Running
  retries: 10
  delay: 2

- name: "[HDFS] Start Hadoop and YARN services inside the master container"
  become: true
  community.docker.docker_container_exec:
    container: "{{ master_container.name }}"
    command: "./start-hadoop.sh"
    chdir: "/root"
    tty: true
  register: hadoop_start_result

- name: "[HDFS] Display Hadoop start output"
  debug:
    var: hadoop_start_result.stdout_lines

- name: "[HDFS] Set permissions on root HDFS directory"
  become: true
  community.docker.docker_container_exec:
    container: "{{ master_container.name }}"
    command: "hdfs dfs -chmod -R 777 /"
    tty: true

- name: "[HDFS] Install required Python packages in all containers"
  become: true
  vars:
    containers: "{{ [master_container.name] + (workers | map(attribute='name') | list) }}"
  loop: "{{ containers }}"
  loop_control:
    label: "{{ item }}"
  community.docker.docker_container_exec:
    container: "{{ item }}"
    command: "pip install pydantic pyyaml influxdb-client 'elasticsearch>=8.7.0,<8.8.0'"
    tty: true

